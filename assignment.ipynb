{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment 'venv (Python 3.13.2)' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "! pwd \n",
    "! ls -al\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#https://jse.amstat.org/v19n3/decock/DataDocumentation.txt\n",
    "df_orig = pd.read_csv('AmesHousing.csv')\n",
    "df_orig.drop(['PID','Order'], axis=1, inplace=True)\n",
    "df_orig.head()\n",
    "# drop ORDER & PID\n",
    "\n",
    "rows, cols = df_orig.shape\n",
    "print(f\"Rows: {rows}, Columns: {cols}\")\n",
    "# Identify missing values\n",
    "missing_values = df_orig.isnull().sum()\n",
    "count_missing=len(np.where(df_orig.isnull())[0])\n",
    "print(f'There are {count_missing} empty values in the dataframe.')\n",
    "df_orig.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment 'venv (Python 3.13.2)' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "# Get a list of categorical and numeric columns\n",
    "#\n",
    "# train_df.dtypes → Returns the data type of each column.\n",
    "# train_df.dtypes != \"object\" → Filters numerical columns (int64, float64).\n",
    "# train_df.dtypes == \"object\" → Filters categorical columns (object data type).\n",
    "# .index.tolist() → Converts the column index to a list for easier use.\n",
    "\n",
    "\n",
    "\n",
    "numerical_df = df_orig.dtypes[df_orig.dtypes != \"object\"].index\n",
    "numerical_column_names=numerical_df.to_list()\n",
    "categorical_df = df_orig.dtypes[df_orig.dtypes == \"object\"].index\n",
    "categorical_column_names=categorical_df.to_list()\n",
    "print('\\nNumerical Columns:')\n",
    "print(numerical_column_names)\n",
    "print('\\nCategorical Columns:')\n",
    "print(categorical_column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Exploration\n",
    "\n",
    "The chart is a histogram with a kernel density estimate (KDE) overlay, showing the distribution of sale prices.\n",
    "Key Elements of the Chart:\n",
    "\n",
    "- X-Axis (Sale Prices): Represents the range of sale prices for the properties.\n",
    "- Y-Axis (Frequency): Indicates how often a sale price falls within a particular range.\n",
    "- Histogram Bars: The bars show the frequency of sales within each price range. Taller bars indicate more frequent sales in that price range.\n",
    "- KDE Curve (Blue Line): This is a smoothed line that estimates the probability density function of sale prices, giving a clearer view of the distribution shape.\n",
    "\n",
    "Insights from the Chart:\n",
    "\n",
    "- Right-Skewed Distribution:\n",
    "    The distribution is right-skewed, meaning there are a few high-priced outliers that pull the tail to the right. Most sales are concentrated in the lower price ranges.\n",
    "\n",
    "- Common Sale Price Range:\n",
    "    The highest concentration of sales appears to be between $100,000 and $200,000, indicating that this is the most common sale price range.\n",
    "\n",
    "- Outliers:\n",
    "    Prices above $400,000 are less frequent, suggesting these are outliers or premium properties.\n",
    "\n",
    "- Long Tail:\n",
    "    The long tail on the right suggests that while luxury or high-end properties exist, they are less common in the dataset.\n",
    "\n",
    "This can reduce our model performance so we will run some outlier removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment 'venv (Python 3.13.2)' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "target_var='SalePrice'\n",
    "transform=False\n",
    "df_orig['SalePrice'].isnull().any()\n",
    "df_orig['SalePrice'].describe()\n",
    "# Here we see the SalePrice descriptive statistics. \n",
    "# The mean is greater than the median which can indicate a positive or \n",
    "# right skewededness. We can confirm this by visualizing the data.\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set the aesthetic style of the plots\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Distribution of Sale Prices\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df_orig['SalePrice'], kde=True, bins=30, color='blue')\n",
    "plt.title('Distribution of Sale Prices')\n",
    "plt.xlabel('Sale Price')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def show_outliers(df):\n",
    "    '''\n",
    "    Insights from Boxplots on SalePrice Outliers\n",
    "\n",
    "    The boxplot of SalePrice revealed the following insights about outliers:\n",
    "\n",
    "        Presence of High-Value Outliers\n",
    "\n",
    "            There are several high-end properties with significantly higher prices than the majority of homes.\n",
    "\n",
    "            These luxury homes may have unique features (e.g., larger lot size, premium locations, high-quality materials) that make them different from the typical market.\n",
    "\n",
    "            If these outliers are not removed or transformed, they could impact model performance by skewing predictions.\n",
    "\n",
    "        Lower-End Outliers Are Less Frequent\n",
    "\n",
    "            There are fewer extreme low-end home prices compared to the high-end outliers.\n",
    "\n",
    "            The minimum home prices seem closer to the majority of the dataset, suggesting that extremely low-value properties are not as unusual.\n",
    "\n",
    "        Distribution is Right-Skewed\n",
    "\n",
    "            The boxplot confirms right-skewness, meaning most homes are in the lower price range, while a smaller number of high-end properties pull the distribution to the right.\n",
    "\n",
    "            A log transformation on SalePrice might be useful to normalize the data.\n",
    "\n",
    "        Possible Need for Outlier Handling\n",
    "\n",
    "            If extreme outliers do not represent typical homes, removing them could help model generalization.'\n",
    "    '''\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.boxplot(x=df[\"SalePrice\"])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    Q1 = df[\"SalePrice\"].quantile(0.25)\n",
    "    Q3 = df[\"SalePrice\"].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    outliers = df[(df[\"SalePrice\"] < lower_bound) | (df[\"SalePrice\"] > upper_bound)]\n",
    "    print(f\"Number of outliers: {len(outliers)}\")\n",
    "\n",
    "def transform_sale_price(target_var,orig_target_var):\n",
    "    \"\"\"\n",
    "    Apply log transformation to the SalePrice column.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): The input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: The DataFrame with the log-transformed SalePrice column.\n",
    "    Why Use Log-Transformed SalePrice as the Target?\n",
    "\n",
    "        Reduces Skewness:\n",
    "\n",
    "            The original SalePrice is right-skewed, meaning that some very expensive houses disproportionately affect the model.\n",
    "\n",
    "            The log transformation normalizes the distribution, making it closer to a normal (Gaussian) distribution, which many models assume.\n",
    "\n",
    "        Improves Model Performance:\n",
    "\n",
    "            Linear regression and tree-based models often perform better with log-transformed targets because they handle proportional relationships rather than absolute differences.\n",
    "\n",
    "        Prevents Overfitting:\n",
    "\n",
    "            Without transformation, models might overly focus on extreme high values, causing instability.\n",
    "\n",
    "            Log transformation ensures that percentage differences matter more than absolute differences.\n",
    "\n",
    "    \"\"\"\n",
    "    # Apply log transformation\n",
    "\n",
    "    # Apply log transformation\n",
    "    log_Sale_price=\"Log_SalePrice\"\n",
    "    df_orig[target_var] = np.log1p(df_orig[orig_target_var])\n",
    "\n",
    "    # Compare distributions before and after transformation\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    # Original SalePrice Distribution\n",
    "    sns.histplot(df_orig[target_var], bins=30, kde=True, ax=axes[0])\n",
    "    axes[0].set_title(\"Original SalePrice Distribution\")\n",
    "\n",
    "    # Log Transformed SalePrice Distribution\n",
    "    sns.histplot(df_orig[target_var], bins=30, kde=True, ax=axes[1], color=\"orange\")\n",
    "    axes[1].set_title(\"Log Transformed SalePrice Distribution\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return df_orig\n",
    "show_outliers(df_orig)\n",
    "if transform:\n",
    "    orig_target_var=target_var\n",
    "    target_var='Log_SalePrice'\n",
    "    df_orig=transform_sale_price(target_var,orig_target_var)\n",
    "print(f'Target var = {target_var}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*House prices by neighborhood*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment 'venv (Python 3.13.2)' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "# Boxplot of SalePrice by Neighborhood\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x=\"Neighborhood\", y=\"SalePrice\", data=df_orig)\n",
    "plt.xticks(rotation=90)  # Rotate labels for readability\n",
    "plt.title(\"House Prices by Neighborhood\")\n",
    "plt.xlabel(\"Neighborhood\")\n",
    "plt.ylabel(\"Sale Price\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Correlation heatmap for numeric features*\n",
    "\n",
    "Analyzing Correlation with SalePrice\n",
    "\n",
    "In this step, we analyze the correlation between SalePrice and all other features. \n",
    "First, we visualize the full correlation matrix using a heatmap to identify strong relationships between features.\n",
    "Next, we calculate and sort the correlation of each feature with SalePrice to identify \n",
    "which features have the most influence on the target variable. This analysis helps in selecting the most \n",
    "important features for the regression model.\n",
    "\n",
    "We can optionally drop values with low correlation values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment 'venv (Python 3.13.2)' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(df_orig.corr(numeric_only=True), cmap=\"coolwarm\", annot=False, linewidths=0.5)\n",
    "plt.title(\"Correlation Heatmap of Ames Housing Features\")\n",
    "plt.show()\n",
    "\n",
    "matrix = np.abs(df_orig[numerical_df].corr())\n",
    "correlation_to_saleprice = matrix['SalePrice'].sort_values(ascending=False)\n",
    "print(\"Correlation of features with 'SalePrice':\")\n",
    "print(correlation_to_saleprice)\n",
    "\n",
    "\n",
    "def do_drop(drop_list):\n",
    "    train_df = train_df.drop(columns=[col for col in drop_list if col in train_df.columns], axis=1)\n",
    "    train_df\n",
    "\n",
    "    # Show only rows that contain NaN values\n",
    "    train_df[train_df.isnull().any(axis=1)]\n",
    "\n",
    "\n",
    "correlation_threshold = 0.1\n",
    "would_drop_list = correlation_to_saleprice[correlation_to_saleprice < correlation_threshold].index.tolist()\n",
    "print(f'Would drop list ={would_drop_list}')\n",
    "drop=False\n",
    "if drop:\n",
    "    do_drop(would_drop_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Define feature groups*\n",
    "\n",
    "I've gone to https://jse.amstat.org/v19n3/decock/DataDocumentation.txt and used a list of location related variables to generate a list of property related variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment 'venv (Python 3.13.2)' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# Created Manually\n",
    "all_location_variables=['MS Zoning',\n",
    "                    'Neighborhood',\n",
    "                    'Condition 1',\n",
    "                    'Condition 2',\n",
    "                    'Lot Frontage',\n",
    "                    'Street',\n",
    "                    'Alley',\n",
    "                    'Lot Shape',\n",
    "                    'Land Contour',\n",
    "                    'Lot Config',\n",
    "                    'Land Slope',\n",
    "                    'Utilities'\n",
    "                    ]\n",
    "\n",
    "# important_location_variables=['MSZoning',\n",
    "#                     'Neighborhood',\n",
    "#                     'Condition1',\n",
    "#                     'Condition2',\n",
    "#                     'Utilities'\n",
    "#                     ]\n",
    "\n",
    "\n",
    "# Get a list of all columns in the dataset\n",
    "column_list = df_orig.columns.tolist()\n",
    "\n",
    "def check_is_sublist(list1, list2):\n",
    "    errors=[]\n",
    "    for i in list1:\n",
    "        if i not in list2:\n",
    "            errors.append(i)\n",
    "    if errors:\n",
    "        print(\"The following variables are not in list2: \",errors)\n",
    "        sys.exit(1)\n",
    "\n",
    "check_is_sublist(all_location_variables,column_list)\n",
    "\n",
    "def create_list_of_property_variables():\n",
    "    property_variables=[]\n",
    "    for c in column_list:\n",
    "        if c not in all_location_variables:\n",
    "            property_variables.append(c)\n",
    "    return property_variables\n",
    "\n",
    "property_variables=create_list_of_property_variables()\n",
    "\n",
    "print(\"Property Variables: \", property_variables)\n",
    "print(\"Location Variables: \", all_location_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handle missing values etc\n",
    "\n",
    "- For columns with numeric values ​​- fill empty values ​​with the column mean.\n",
    "- For columns with categorical values ​​- we will fill empty values ​​with the most common value in the column\n",
    "\n",
    "Common Thresholds\n",
    "\n",
    "- Above 60-70% missing: Consider dropping unless the column is critical.\n",
    "- 30-60% missing: Consider imputing if the feature is valuable.\n",
    "- Below 30% missing: Usually worth imputing rather than dropping.\n",
    "\n",
    "In this case\n",
    "- 'Pool QC' (99.6%), 'Misc Feature' (96.4%), 'Alley' (93.2%), and 'Fence' (80.5%) → Likely not useful and can be dropped.- \n",
    "- 'Fireplace Qu' (48.6%) → Might be worth imputing if fireplaces impact home values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment 'venv (Python 3.13.2)' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "# Identify missing values\n",
    "missing_values = df_orig.isnull().sum()\n",
    "count_missing=len(np.where(df_orig.isnull())[0])\n",
    "print(f'There are {count_missing} empty values in the dataframe.')\n",
    "# Filter only columns with missing values\n",
    "missing_values = missing_values[missing_values > 0]\n",
    "\n",
    "# Create a DataFrame with missing count and percentage\n",
    "missing_values_df = pd.DataFrame({\n",
    "    'Column': missing_values.index,\n",
    "    'Missing Count': missing_values.values,\n",
    "    'Missing Percentage': (missing_values.values / len(df_orig)) * 100\n",
    "})\n",
    "\n",
    "# Sort by highest missing percentage\n",
    "missing_values_df = missing_values_df.sort_values(by='Missing Percentage', ascending=False)\n",
    "\n",
    "# Display the result\n",
    "print(missing_values_df)\n",
    "\n",
    "rows, cols = df_orig.shape\n",
    "print(f\"Rows: {rows}, Columns: {cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment 'venv (Python 3.13.2)' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "\n",
    "def do_drop(df):\n",
    "    threshold = 0.80  # 80%\n",
    "    cols_to_drop = missing_values_df[missing_values_df['Missing Percentage'] > (threshold * 100)]['Column'].tolist()\n",
    "    print(f\"Columns to drop: {cols_to_drop}\")\n",
    "    df_cleaned = df.drop(columns=cols_to_drop)\n",
    "    return df_cleaned\n",
    "\n",
    "def do_impute(df):\n",
    "    # Impute missing values\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object':  # Categorical columns\n",
    "            df[col].fillna(df[col].mode()[0], inplace=True)\n",
    "        else:  # Numeric columns\n",
    "            df[col].fillna(df[col].mean(), inplace=True)\n",
    "    return df\n",
    "df=df_orig\n",
    "\n",
    "\n",
    "\n",
    "drop=False\n",
    "df=df_orig\n",
    "if drop:\n",
    "    df=do_drop(df)\n",
    "df_cleaned=do_impute(df)\n",
    "\n",
    "\n",
    "# Identify missing values\n",
    "missing_values = df_cleaned.isnull().sum()\n",
    "count_missing=len(np.where(df_orig.isnull())[0])\n",
    "print(f'There are {count_missing} empty values in the dataframe.')\n",
    "rows, cols = df_cleaned.shape\n",
    "print(f\"Rows: {rows}, Columns: {cols}\")\n",
    "# RETURN DF_CLEANED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode Categorical Variables\n",
    "\n",
    "NOTE: WE HAVE ADDED LOTS OF EXTRA COLUMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment 'venv (Python 3.13.2)' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "train_df=df_cleaned\n",
    "\n",
    "# Identify categorical features\n",
    "categorical_cols = categorical_column_names\n",
    "\n",
    "# Initialize OneHotEncoder\n",
    "encoder = OneHotEncoder(drop='first', sparse_output=False)\n",
    "\n",
    "# Fit-transform and convert to dataframe\n",
    "encoded_array = encoder.fit_transform(train_df[categorical_cols])\n",
    "encoded_df = pd.DataFrame(encoded_array, columns=encoder.get_feature_names_out(categorical_cols))\n",
    "\n",
    "# Concatenate with numerical features and drop original categorical columns\n",
    "df_encoded = pd.concat([train_df.drop(columns=categorical_cols), encoded_df], axis=1)\n",
    "df_encoded.head()\n",
    "rows, cols = df_encoded.shape\n",
    "print(f\"Rows: {rows}, Columns: {cols}\")\n",
    "df_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Training and Test Sets\n",
    "\n",
    "Key Parameters in train_test_split\n",
    "\n",
    "- test_size=0.2 → 20% of the data goes to the test set, and 80% to the training set.\n",
    "- random_state=42 → Ensures reproducibility of the split.\n",
    "- X contains the independent variables (features).\n",
    "- y contains the dependent variable (target)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'target_var' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Define target variable (House Price)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m target \u001b[38;5;241m=\u001b[39m \u001b[43mtarget_var\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget Variable:\u001b[39m\u001b[38;5;124m\"\u001b[39m, target)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Define features (X) and target variable (y)\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'target_var' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Define target variable (House Price)\n",
    "target = target_var\n",
    "\n",
    "print(\"Target Variable:\", target)\n",
    "\n",
    "# Define features (X) and target variable (y)\n",
    "X = df_encoded.drop(columns=[target])  # Independent variables\n",
    "y = df_encoded[target]  # Dependent variable\n",
    "\n",
    "# Split into training (80%) and test (20%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest performs better than Linear Regression on all three metrics:\n",
    "\n",
    "    Mean Absolute Error (MAE): Measures the average absolute difference between predicted and actual prices.\n",
    "\n",
    "        Random Forest (15,820.78) < Linear Regression (16,637.43) → Lower is better.\n",
    "\n",
    "    Root Mean Squared Error (RMSE): Measures the standard deviation of prediction errors.\n",
    "\n",
    "        Random Forest (26,812.54) < Linear Regression (36,011.37) → Lower is better.\n",
    "\n",
    "    R² Score: Measures how well the model explains variance in the target variable.\n",
    "\n",
    "        Random Forest (0.9103) > Linear Regression (0.8383) → Closer to 1 is better.\n",
    "\n",
    "Insights:\n",
    "\n",
    "    Random Forest outperforms Linear Regression significantly, especially in RMSE and R², meaning it generalizes better and captures non-linear relationships effectively.\n",
    "\n",
    "    Linear Regression might struggle with complex relationships in the data, especially if interactions between features and non-linearity exist.\n",
    "\n",
    "    If interpretability is important, Linear Regression is easier to explain, while Random Forest is more of a \"black box.\"\n",
    "\n",
    "Next Steps:\n",
    "\n",
    "    Feature Engineering: If you want to improve Linear Regression, consider interactions, polynomial terms, or feature selection.\n",
    "\n",
    "    Hyperparameter Tuning: For Random Forest, tuning parameters (like the number of trees, max depth, etc.) could improve results.\n",
    "\n",
    "    Exploring Other Models: Try Gradient Boosting (e.g., XGBoost, LightGBM) if you want even better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment 'venv (Python 3.13.2)' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Function to train and evaluate models\n",
    "def train_and_evaluate(X_train, X_test, y_train, y_test, model, model_name):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Evaluate model performance\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    return {\"Model\": model_name, \"MAE\": mae, \"RMSE\": rmse, \"R²\": r2}\n",
    "\n",
    "# Define models\n",
    "models = [\n",
    "    (LinearRegression(), \"Linear Regression (All Features)\"),\n",
    "    (RandomForestRegressor(n_estimators=100, random_state=42), \"Random Forest (All Features)\"),\n",
    "]\n",
    "\n",
    "# Store results\n",
    "results = []\n",
    "\n",
    "# Train and evaluate models\n",
    "for model, name in models:\n",
    "    results.append(train_and_evaluate(X_train, X_test, y_train, y_test, model, name))\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Importance Analysis (Random Forest)\n",
    "\n",
    "Key Findings\n",
    "\n",
    "The most important features in predicting house prices are:\n",
    "\n",
    "- Gr Liv Area (Above-Ground Living Area)\n",
    "- Overall Qual (Overall Material and Finish Quality)\n",
    "- Total Bsmt SF (Total Basement Area)\n",
    "- Garage Cars (Number of Garage Spaces)\n",
    "- Year Built (Construction Year)\n",
    "\n",
    "Location-related features (e.g., Neighborhood, Lot Frontage) have lower importance than property-related features.\n",
    "\n",
    "Property features dominate price prediction, confirming that aspects like size, quality, and amenities matter more than location.\n",
    "\n",
    "Location still has some influence, but it does not significantly outperform direct property characteristics.\n",
    "\n",
    "AI models trained on housing data tend to prioritize tangible home characteristics over neighborhood effects, which can be important in real estate valuation, lending, and urban planning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment 'venv (Python 3.13.2)' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Train Random Forest on All Features\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "feature_importances = pd.DataFrame({\n",
    "    \"Feature\": X_train.columns,\n",
    "    \"Importance\": rf_model.feature_importances_\n",
    "}).sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "# Display feature importance rankings\n",
    "print(feature_importances.head(15))\n",
    "\n",
    "# Visualizing Feature Importance\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=\"Importance\", y=\"Feature\", data=feature_importances.head(15))\n",
    "plt.title(\"Top 15 Feature Importances (Random Forest)\")\n",
    "plt.xlabel(\"Importance Score\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize Feature Importance Using SHAP\n",
    "\n",
    "\n",
    "Key Insights from the SHAP Summary Plot\n",
    "\n",
    "    Most Important Features\n",
    "\n",
    "        Overall Qual (Overall Material and Finish Quality) has the highest impact on price predictions.\n",
    "\n",
    "        Gr Liv Area (Above-Ground Living Area) and Total Bsmt SF (Total Basement Size) are also highly influential.\n",
    "\n",
    "        1st Flr SF, Garage Area, and Year Built contribute significantly.\n",
    "\n",
    "    How SHAP Values Work\n",
    "\n",
    "        Each dot represents a single data point.\n",
    "\n",
    "        Red dots (high feature values) are mostly on the right, showing a positive impact on price.\n",
    "\n",
    "        Blue dots (low feature values) are mostly on the left, indicating a negative effect.\n",
    "\n",
    "        Example: Higher Overall Qual (red) increases house prices, while lower values (blue) decrease them.\n",
    "\n",
    "    Limited Impact of Location-Based Features\n",
    "\n",
    "        Location-related variables such as Neighborhood are missing from the top-ranking features.\n",
    "\n",
    "        This suggests that property-related attributes dominate price predictions, even with location data present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment 'Python' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "import shap\n",
    "# import matplotlib.pyplot as plt\n",
    "# import pandas as pd\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# # Define and train the Random Forest model without location features\n",
    "# rf_model_no_loc = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "# rf_model_no_loc.fit(X_train_no_location, y_train)\n",
    "\n",
    "# Train Random Forest model on all features (including location)\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "# Initialize SHAP explainer\n",
    "explainer = shap.Explainer(rf_model, X_train)\n",
    "\n",
    "# Compute SHAP values for the test set\n",
    "shap_values = explainer(X_test)\n",
    "shap.summary_plot(shap_values, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove Location Features from Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment 'Python' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "# Create a dataset without location features\n",
    "X_train_no_location = X_train.drop(columns=[col for col in all_location_variables if col in X_train.columns], errors='ignore')\n",
    "X_test_no_location = X_test.drop(columns=[col for col in all_location_variables if col in X_test.columns], errors='ignore')\n",
    "X_test_no_location\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Models Without Location Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment 'Python' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Function to train and evaluate models\n",
    "def train_and_evaluate(X_train, X_test, y_train, y_test, model, model_name):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Evaluate model performance\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    return {\"Model\": model_name, \"MAE\": mae, \"RMSE\": rmse, \"R²\": r2}\n",
    "\n",
    "# Define models to test without location features\n",
    "models_no_location = [\n",
    "    (LinearRegression(), \"Linear Regression (No Location Features)\"),\n",
    "    (RandomForestRegressor(n_estimators=100, random_state=42), \"Random Forest (No Location Features)\")\n",
    "]\n",
    "\n",
    "# Store results\n",
    "results_no_location = []\n",
    "\n",
    "# Train and evaluate models without location features\n",
    "for model, name in models_no_location:\n",
    "    results_no_location.append(train_and_evaluate(X_train_no_location, X_test_no_location, y_train, y_test, model, name))\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_no_location_df = pd.DataFrame(results_no_location)\n",
    "print(results_no_location_df)\n",
    "\n",
    "# Combine both results into a single table\n",
    "comparison_df = pd.concat([results_df, results_no_location_df], ignore_index=True)\n",
    "\n",
    "# Display the combined results\n",
    "print(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best Performing Model:\n",
    "\n",
    "Random Forest (No Location Features)\n",
    "\n",
    "- Lowest RMSE (26,671) → Best overall predictive accuracy\n",
    "- Highest R² (0.9112) → Best fit to the data\n",
    "- Lower MAE (15,706) → Less absolute error in price predictions\n",
    "\n",
    "Key Takeaways\n",
    "- Random Forest consistently outperforms Linear Regression, showing that house prices depend on non-linear relationships.\n",
    "- Removing location features has minimal impact on Random Forest performance, meaning property characteristics drive house prices more than location.\n",
    "- Random Forest without location features is slightly better than with location features, indicating that the model learns enough from property attributes alone."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
